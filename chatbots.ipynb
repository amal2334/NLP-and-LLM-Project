{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhPYJPp2jgMn2vQULzovQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amal2334/NLP-and-LLM-Project/blob/main/chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Specialized Chatbots with LangChain: From Prompts to RAG**"
      ],
      "metadata": {
        "id": "_QEgc_rzUMpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the keys\n",
        "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "# Check if loaded (optional for testing)\n",
        "print(\"HuggingFace key loaded:\", hf_token is not None)\n",
        "print(\"Groq key loaded:\", groq_api_key is not None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auXCq7Xc3v5C",
        "outputId": "323928a8-a2e2-4416-d6f8-aba8df47baa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.0)\n",
            "HuggingFace key loaded: True\n",
            "Groq key loaded: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Table of content**\n",
        "In this notebook, we'll explore:\n",
        "\n",
        "- **Introduction**\n",
        "\n",
        "- **Setup & Downloads**\n",
        "  - Install required libraries\n",
        "\n",
        "  - Import core modules\n",
        "\n",
        "  - Set API keys and environment\n",
        "\n",
        "- **1. Human Text Generation with Context**\n",
        "  - Mini Chatbot: ‚úàÔ∏è Travel Assistant by Recommending  travel ideas based on user input\n",
        "\n",
        "- **2. Conversation Management and Multi-User Support**\n",
        "  -  Mini Chatbot: üìÖ Personal Productivity Coach by Helping  users plan goals and routines with memory per user\n",
        "\n",
        "- **3. Prompt Engineering**\n",
        "  - Mini Chatbot: üõí Shopping Assistant by Helping  users choose the right product based on preferences\n",
        "\n",
        "- **4. Multilingual Capabilities with Chat Histor**y\n",
        "   - Mini Chatbot: üó£Ô∏è Language Learning Assistant with Memory by Helping users learn and practice Arabic, English, etc.\n",
        "\n",
        "- **5. Token Trimming for Long Conversations**\n",
        "   - Mini Chatbot: üìû Customer Support Assistant by Assisting  users in long chats without losing coherence\n",
        "\n",
        "- **6. Retrieval-Augmented Generation (RAG)**\n",
        "    - Mini Chatbot: üìñ AI Study Assistant (RAG + Memory) and Answers research questions using real blog content.\n",
        "    - Tracks chat history for follow-up questions\n",
        "-  Conclusion\n",
        "- Final Thoughts\n"
      ],
      "metadata": {
        "id": "5-HqOKrZU_fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "- In this project we will be building a modern Q&A chatbot using Large Language Models (LLMs) and the LangChain framework in which we will explore the field of generative AI and creating a functional, context-aware conversational system."
      ],
      "metadata": {
        "id": "Y0aqMDCxI5mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview**\n",
        "- Conversational AI is transforming how humans interact with computers, enabling more natural and intuitive interfaces. The techniques covered in this notebook form the foundation of many modern AI applications, from customer service bots to virtual assistants and knowledge management systems."
      ],
      "metadata": {
        "id": "GeKoU5pNVOLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Envirenment Setup**\n",
        "The key libraries we'll use include:\n",
        "- `langchain`: Core framework for working with LLMs\n",
        "- `langchain_community`: Community-contributed components and integrations\n",
        "- `langchain_core`: Essential components like message types and prompt templates\n",
        "- `langchain_chroma`: Vector database integration for knowledge retrieval\n"
      ],
      "metadata": {
        "id": "6zbMlXfxYMmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "- Throughtout this notebook , we'll be using API key from Groq and Hugging Face"
      ],
      "metadata": {
        "id": "tg0VpxE14sF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq\n",
        "!pip install dotenv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from langchain_groq import ChatGroq\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain_chroma\n",
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20V3VPkW7VDS",
        "outputId": "0065de73-aa59-4ab3-a482-89342e1b6187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.63)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.26.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.14.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.44)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.4.0)\n",
            "Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.63)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.44)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: langchain-core>=0.3.60 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.63)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.0.12)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (2.11.5)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.24.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb>=1.0.9->langchain_chroma) (0.45.3)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (0.3.44)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.60->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.3.60->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core>=0.3.60->langchain_chroma) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain_chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (0.32.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.1.0)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain_chroma) (1.1.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.4.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain_chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihPsadMyECri",
        "outputId": "9e7bce17-e85a-4bb6-cb85-a58e62cd826d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7d89dc573dd0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7d89dc56da90>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "The foundation of our chatbot is the Large Language Model (LLM). In this project, we're using **Gemma2 9B-IT**, a powerful instruction-tuned model accessible through Ollama.\n",
        "\n",
        "**What is Gemma2?**:\n",
        "Gemma2 is Google's lightweight, state-of-the-art open model that delivers strong performance while being efficient enough to run on consumer hardware. The \"9B\" refers to the number of parameters (9 billion), and \"IT\" indicates it's instruction-tuned, meaning it's specifically optimized for following instructions and generating helpful responses.\n",
        "\n",
        "**What is Ollama?**\n",
        "Ollama is a framework that simplifies running open-source LLMs locally. It handles model downloading, optimization, and inference, providing a simple API for applications to interact with these models.\n",
        "\n"
      ],
      "metadata": {
        "id": "_gVL0qqrYjij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1-Human Text Generation with Context**"
      ],
      "metadata": {
        "id": "jVU782OQ3dcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "\n",
        "        HumanMessage(content=\"Hi , My name is Amal and I am a data analyst , later on inshallah a data scientis\"),\n",
        "        AIMessage(content=\"Hello Amal! It's nice to meet you. \\n\\nAs a data  Analyst, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI and data analysis.\\n\"),\n",
        "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8r7Zd54ESn3",
        "outputId": "a6ba8f0c-86ea-40d6-ba7b-6d030be81548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"You are Amal, and you are a data analyst, aspiring to become a data scientist! üòä  \\n\\nIs there anything else you'd like to tell me about yourself or your work?  I'm happy to chat. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 109, 'total_tokens': 159, 'completion_time': 0.090909091, 'prompt_time': 0.005150232, 'queue_time': 0.09209899499999999, 'total_time': 0.096059323}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--5e117fe3-9562-4bdc-83f5-83917a8cd3ca-0', usage_metadata={'input_tokens': 109, 'output_tokens': 50, 'total_tokens': 159})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant\"),\n",
        "        HumanMessage(content=\"Hi , My name is Ajay and I am a data analyst\"),\n",
        "        AIMessage(content=\"Hello Ajay ! It's nice to meet you. \\n\\nAs a data Analyst, what kind of projects are you working on these days? \\n\\nI'm always eager to learn more about the exciting work being done in the field of AI and data analysis.\\n\"),\n",
        "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNeAxcw5D3l5",
        "outputId": "559d1aaf-31c0-428d-a5b0-e3d1ee63b1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You are Ajay, a data analyst!  I remember üòä  \\n\\nHow can I help you with your data analysis work today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 103, 'total_tokens': 132, 'completion_time': 0.052727273, 'prompt_time': 0.00569996, 'queue_time': 0.108991695, 'total_time': 0.058427233}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--3fb2f998-79e9-48ae-a61b-2d9f689640f1-0', usage_metadata={'input_tokens': 103, 'output_tokens': 29, 'total_tokens': 132})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini chatbot :Travel Assistant**"
      ],
      "metadata": {
        "id": "twSb-1Wa21sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a smart travel assistant who helps people plan their vacations.\"),\n",
        "        HumanMessage(content=\"Hi, I‚Äôm Sarah and I‚Äôd like to plan a trip to Italy next month.\"),\n",
        "        AIMessage(content=\"Hi Sarah! That sounds exciting. I'd love to help you plan your trip to Italy. Do you have any cities or activities in mind?\"),\n",
        "        HumanMessage(content=\"What‚Äôs my name and where do I want to go?\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyUD5gBDKvbv",
        "outputId": "5f36e354-3bcc-4b4e-f13f-ae8c6de251f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You said your name is Sarah and you want to go to Italy! üáÆüáπ \\n\\nAnything else I can help you with to start planning your trip? Do you have a particular region or city in mind?  Any must-see sights or activities? What kind of budget are you working with?  The more information you give me, the better I can help! üòä\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 95, 'total_tokens': 174, 'completion_time': 0.143636364, 'prompt_time': 0.004291292, 'queue_time': 0.09223461699999999, 'total_time': 0.147927656}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c3db16b4-f192-4c0a-b4b8-cb2c476e5b1e-0', usage_metadata={'input_tokens': 95, 'output_tokens': 79, 'total_tokens': 174})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "Message Types in Conversational AI\n",
        "\n",
        "In a conversational system, we need to distinguish between different types of messages. LangChain provides several message classes to represent different participants in a conversation:\n",
        "\n",
        "1. **HumanMessage**: Represents messages from the user\n",
        "2. **AIMessage**: Represents responses from the AI assistant\n",
        "3. **SystemMessage**: Contains instructions or context for the AI that aren't part of the visible conversation"
      ],
      "metadata": {
        "id": "Fpp-x5AKY4KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2-Conversation management and Multi-User Support**\n",
        "- We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore."
      ],
      "metadata": {
        "id": "ZFNA5WskE5_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store={}\n",
        "\n",
        "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id]=ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
      ],
      "metadata": {
        "id": "m5M1PXCME9xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config={\"configurable\":{\"session_id\":\"chat1\"}}"
      ],
      "metadata": {
        "id": "eR4MMDkeFCMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi , My name is Amal and I am a data analyst\")],\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "8Af95epjFFV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "R3ivMmc7FWG-",
        "outputId": "87f24b95-1989-4936-f5dd-dc7003a6f89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Amal! It's nice to meet you.  \\n\\nAs a data analyst, what kind of work do you enjoy doing the most? \\n\\nDo you have any interesting projects you're working on right now? I'm always eager to learn more about how people use data to solve problems and uncover insights.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Te__nqFrzk",
        "outputId": "e3d2238f-00eb-4071-a23c-f293c186f63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Amal. üòä \\n\\nI remembered that from our introduction!  How can I help you today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 103, 'total_tokens': 129, 'completion_time': 0.047272727, 'prompt_time': 0.005109321, 'queue_time': 0.066195094, 'total_time': 0.052382048}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--75a4ff08-dfe7-4657-844d-d958d26f0feb-0', usage_metadata={'input_tokens': 103, 'output_tokens': 26, 'total_tokens': 129})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## change the config-->session id\n",
        "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Whats my name\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MQ4WY8ABFxiz",
        "outputId": "fd86697a-ce2d-403e-bd2c-f2e98d58d753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI, I do not have access to any personal information about you, including your name. If you'd like to tell me your name, I'd be happy to know! üòä  \\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "- Model memory allows the AI to remember what was said earlier in a conversation, just like talking to a person who listens and responds based on context. In LangChain, this memory is stored using a session ID.\n",
        "- When you stay in the same session, the model can recall your name, role, or anything you previously mentioned. But when you change the session ID, it‚Äôs like starting a brand-new chat.\n",
        "- the model has no memory of past messages. So, memory works only within the same session, and changing the session resets the conversation history."
      ],
      "metadata": {
        "id": "d-HbY62WJ_Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hey My name is Ajay\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wN7PVaooF_ZK",
        "outputId": "264c3f09-b00b-4bdd-8131-7f239a79384a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Ajay, it's nice to meet you! üëã  \\n\\nIs there anything I can help you with today? üòä  \\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Whats my name\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IM-BFqRwGCr1",
        "outputId": "c0527574-40dd-47ac-d615-59c6d939e166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Ajay! üòä  \\n\\nI remember that you told me earlier.  \\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "- If we switch sessions, it‚Äôs like talking to the AI for the first time.\n",
        "\n",
        "- We must build memory step by step within each session"
      ],
      "metadata": {
        "id": "3bdioGZGLzNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini Chatbot: Personal Productivity Coach**"
      ],
      "metadata": {
        "id": "novk_jD-871n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a personal productivity coach. \"\n",
        "    \"Help users build routines, focus on goals, and stay productive. \"\n",
        "    \"Remember what each user tells you during the session.\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "\n",
        "with_memory_chain = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "user1_session = \"amal123\"\n",
        "user2_session = \"Ajay456\"\n",
        "\n",
        "# Amal's message\n",
        "response1 = with_memory_chain.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"give me some advices on how i can  build a Fashion brand\")]},\n",
        "    config={\"configurable\": {\"session_id\": user1_session}}\n",
        ")\n",
        "\n",
        "# Ajay 's message\n",
        "response2 = with_memory_chain.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What would a meaningful life look like to you.\")]},\n",
        "    config={\"configurable\": {\"session_id\": user2_session}}\n",
        ")\n",
        "\n",
        "response1.content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "wsu-L4Ds9C5C",
        "outputId": "5daacc64-7650-4bd4-e8ef-4867a32edb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It\\'s fantastic that you\\'re looking to build a fashion brand! That\\'s a creative and exciting goal. To help you get started, let\\'s break it down into manageable steps and build a productive routine around it.  \\n\\n**First, let\\'s talk about your vision:**\\n\\n* **What makes your brand unique?** What\\'s your style, your target audience, your values?  \\n* **What kind of fashion are you passionate about?**  Clothing, accessories, footwear?  \\n* **Do you have any specific niches in mind?** Sustainable fashion, streetwear, luxury wear?\\n\\n**Next, let\\'s focus on building a routine:**\\n\\n* **Dedicated Time:** Set aside specific time each day or week to work on your brand. Even 30 minutes can be productive! Treat it like an important appointment you can\\'t miss.\\n* **Actionable Tasks:** Break down your big goals into smaller, actionable tasks. For example, instead of \"design a collection,\" start with \"research fabrics\" or \"sketch 5 outfit ideas.\"\\n* **Prioritization:** Use a to-do list or a planner to prioritize tasks. Focus on the most important ones first and delegate or postpone less urgent ones.\\n\\n**Here are some specific steps you can take:**\\n\\n1. **Market Research:**\\n    * **Analyze your target audience:** Who are they? What are their needs and wants? Where do they hang out online and offline?\\n    * **Research competitors:**  What are they doing well? What are their weaknesses? How can you differentiate yourself?\\n\\n2. **Brand Development:**\\n    * **Create a brand identity:** This includes your logo, color palette, typography, and overall aesthetic.\\n    * **Develop your brand story:** What\\'s the inspiration behind your brand? What values do you stand for?\\n\\n3. **Product Development:**\\n    * **Design your first collection:** Start small and focus on creating high-quality pieces that reflect your brand identity.\\n    * **Source materials:** Find ethical and sustainable suppliers for your fabrics and trims.\\n    * **Consider production options:** Will you manufacture your own clothes or work with a factory?\\n\\n4. **Marketing and Sales:**\\n    * **Build an online presence:** Create a website and social media profiles for your brand.\\n    * **Develop a marketing strategy:** How will you reach your target audience?\\n    * **Explore sales channels:** Will you sell online, in boutiques, or at pop-up shops?\\n\\n5. **Financial Planning:**\\n    * **Create a budget:**  Estimate your startup costs and ongoing expenses.\\n    * **Explore funding options:** Will you bootstrap your business, seek investors, or apply for loans?\\n\\nRemember, building a fashion brand takes time, effort, and dedication. Be patient, stay focused, and celebrate your successes along the way. \\n\\n\\nI\\'ll be here to help you every step of the way.  What\\'s the first step you want to tackle today?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "lKWbgxD09tGd",
        "outputId": "78c06a92-8c98-4967-94f4-5a56dea23860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"For me, a meaningful life is about continuous growth, learning, and making a positive impact. It's about using my abilities to help others, to create something new and valuable, and to leave the world a little better than I found it.  \\n\\nAs a personal productivity coach, I get to witness that kind of meaning unfold in the lives of others. Helping someone break through a barrier, achieve a long-held goal, or simply feel more in control of their time and energy is incredibly fulfilling.\\n\\nNow, tell me about you. What does a meaningful life look like to you? What are you passionate about? What are some things you'd like to achieve?  Let's work together to make those dreams a reality.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3- Prompt Engineering**"
      ],
      "metadata": {
        "id": "LCSlBccZGZD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your ability\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain=prompt|model"
      ],
      "metadata": {
        "id": "9WdV0qV6GcDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Amal\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rpdj7i-GfNf",
        "outputId": "d24080e7-559a-4af5-d294-d0e42fde4e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello Amal!  It's nice to meet you. \\n\\nI'm here to help. What can I do for you? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 30, 'total_tokens': 64, 'completion_time': 0.061818182, 'prompt_time': 0.002158907, 'queue_time': 0.09089634099999999, 'total_time': 0.063977089}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--67aac1d3-da33-4545-a70a-95b6aefbc4b8-0', usage_metadata={'input_tokens': 30, 'output_tokens': 34, 'total_tokens': 64})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
      ],
      "metadata": {
        "id": "BUhhl6iyGpta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi My name is Amal\")],\n",
        "    config=config\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaIW6r_NGuTY",
        "outputId": "1dcc6bec-b7a5-470b-c308-2540532e9202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Amal! It's nice to meet you. üòä \\n\\nWhat can I do for you today? I'm ready for your questions!  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 30, 'total_tokens': 65, 'completion_time': 0.063636364, 'prompt_time': 0.002145457, 'queue_time': 0.090929571, 'total_time': 0.065781821}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--374a453e-9814-48eb-9120-6b27ef8638dc-0', usage_metadata={'input_tokens': 30, 'output_tokens': 35, 'total_tokens': 65})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5XPp-9NMG38i",
        "outputId": "4ddbec79-fca9-487f-d848-2e15c9adbd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Amal.  I remember! üòä  \\n\\n\\nIs there anything else I can help you with?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini chatbot: Shopping Assistant**"
      ],
      "metadata": {
        "id": "CpSAmvDEOsTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant. Answer all the questions to the best of your ability\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model\n",
        "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"shop_user1\"}}\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi, I‚Äôm looking for a new laptop for programming.\")]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi I care about GPU and RAM \")]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3ic5nVlSlb3",
        "outputId": "d651079c-9c80-4fb3-f3fd-2dd2a36b538e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great! Knowing you prioritize GPU and RAM helps a lot.\n",
            "\n",
            "To give you the most useful suggestions, could you tell me:\n",
            "\n",
            "* **What's your budget range for this laptop?**  GPUs and RAM can significantly impact the price. \n",
            "* **What kind of programming will you primarily be doing?**  \n",
            "\n",
            "    *  If it's **data science, machine learning, or AI**, you'll want a powerful GPU. \n",
            "    *  For **game development or 3D graphics**, a dedicated GPU is also essential.\n",
            "    *  If it's more **web development or general coding**, a less powerful GPU might suffice.\n",
            "\n",
            "* **How much RAM do you ideally need?** 16GB is becoming standard for modern programming, but some tasks might benefit from 32GB or more.\n",
            "\n",
            "\n",
            "Let me know these details, and I can recommend some excellent laptops with the GPU and RAM power you're looking for!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4-multilingual capabilities with chat History**"
      ],
      "metadata": {
        "id": "fiXM-UtQZYOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ch\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "-fcZGoBxG8iI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d311da7e-ad8f-4606-8bfe-05f8575f593c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-df6ab57ed1d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m prompt = ChatPromptTemplate.from_messages(\n\u001b[1;32m      3\u001b[0m     [\n\u001b[1;32m      4\u001b[0m         (\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Amal\")],\"language\":\"Arabic\"})\n",
        "response.content"
      ],
      "metadata": {
        "id": "saDHQ0QPHEhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Ajay\")],\"language\":\"Telugu\"})\n",
        "response.content"
      ],
      "metadata": {
        "id": "wESvaHhTHLea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is selcuk\")],\"language\":\"Turkish\"})\n",
        "response.content"
      ],
      "metadata": {
        "id": "fcsBdRPIHTSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Amal\")],\"language\":\"English\"})\n",
        "response.content"
      ],
      "metadata": {
        "id": "CRzDE14MIMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history=RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\"\n",
        ")"
      ],
      "metadata": {
        "id": "iHjqM8kmIYVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
        "repsonse=with_message_history.invoke(\n",
        "    {'messages': [HumanMessage(content=\"Hi,I am Amal\")],\"language\":\"English\"},\n",
        "    config=config\n",
        ")\n",
        "repsonse.content"
      ],
      "metadata": {
        "id": "SwPYJIG1IhdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"English\"},\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "lns5nMjXIzd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "id": "a04ZJIhNI-XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini ChatBot :Language learning Assistant with Memory**"
      ],
      "metadata": {
        "id": "Fwa7TaOjeJ5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We‚Äôve built a smart language learning tutor that responds in your chosen language **(like English or Spanish)** and remembers what you‚Äôve learned during the session. This tutor can help you practice vocabulary, correct your mistakes, and track your progress over time using memory. It's like having a personal language coach who understands you and guides you step by step."
      ],
      "metadata": {
        "id": "i6_GhOrbfY4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a supportive language tutor. Respond in {language}. Encourage and correct the student. Remember what they learned.\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\")\n",
        "])\n",
        "\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"english_learning_amal\"}}\n"
      ],
      "metadata": {
        "id": "0uTR2T4zeU5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"How do I say 'libro 'in English?\")],\n",
        "        \"language\": \"English\"\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "print(\"Bot:\", response1.content)\n"
      ],
      "metadata": {
        "id": "J8nZF-SyemC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What new word did I learn?\")],\n",
        "        \"language\": \"English\"\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "print(\"Bot:\", response2.content)\n"
      ],
      "metadata": {
        "id": "xzUQy53ceyOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5-Token Trimming for Long Conversation**"
      ],
      "metadata": {
        "id": "aUOSYhU1JLuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage,trim_messages\n",
        "trimmer=trim_messages(\n",
        "    max_tokens=45,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm Amal\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "id": "k85zDbUnJRx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain=(\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        "\n",
        ")\n",
        "\n",
        "response=chain.invoke(\n",
        "    {\n",
        "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
        "    \"language\":\"English\"\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "id": "psacYY9nJc4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "id": "D3aN7zpIJlHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "- The model remembers the math problem (\"what‚Äôs 2 + 2\") because that message appears near the end of the conversation, and the  trimming strategy keeps the most recent messages within a 45-token limit. Since the math question and its answer are still within that limit, they are included when the model is called.\n",
        "- In contrast, earlier messages like \"I like vanilla ice cream\" are further back in the history and may be trimmed out, so the model can no longer see or remember them.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJ2RJSXakQcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets wrap this in the MEssage History\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")\n",
        "config={\"configurable\":{\"session_id\":\"chat5\"}}"
      ],
      "metadata": {
        "id": "dqI8SUsnJqA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "id": "e6b2OacnJs5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "- The reason the model replied:‚ÄúAs an AI, I don‚Äôt have memory.‚Äùis because this was the first time the session ID \"chat5\" was used.\n",
        "\n"
      ],
      "metadata": {
        "id": "CbQkGA1umVAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini Customer Support chatbot**"
      ],
      "metadata": {
        "id": "EjmDKDovl0dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=100,\n",
        "    strategy=\"last\",  # Keep the most recent messages\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False\n",
        ")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful customer support assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\")\n",
        "])\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful customer support assistant.\"),\n",
        "    HumanMessage(content=\"Hi, I need help with a refund.\"),\n",
        "    AIMessage(content=\"Sure, can you share your order number?\"),\n",
        "    HumanMessage(content=\"It's #12345\"),\n",
        "    AIMessage(content=\"Thanks. I see your order.\"),\n",
        "    HumanMessage(content=\"Also, I was charged twice.\"),\n",
        "    AIMessage(content=\"Sorry to hear that. We'll fix it.\"),\n",
        "    HumanMessage(content=\"And I never received the item.\"),\n",
        "    AIMessage(content=\"Let me check that for you.\"),\n",
        "    HumanMessage(content=\"Can I still get my money back?\"),\n",
        "]\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"messages\": messages + [HumanMessage(content=\"When can I expect a refund?\")],\n",
        "})\n",
        "\n",
        "print(\"Bot:\", response.content)\n"
      ],
      "metadata": {
        "id": "bFGkRWJPJ3Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this customer **support chatbot**, token **trimming** is used to keep the conversation within a safe limit by automatically removing older messages and preserving the most recent ones.\n",
        "- This ensures that the chatbot focuses only on the relevant part of the conversation, especially during long chats.\n",
        "- When combined with message history, the system can track each user‚Äôs session separately while still **applying token limits** to avoid exceeding the model‚Äôs capacity.\n",
        "- This approach keeps the **chatbot efficient, responsive, and context-aware**, even during extended support interactions."
      ],
      "metadata": {
        "id": "rYTkF8HRm-g7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6- Retrieval-Augmented Generation (RAG)**"
      ],
      "metadata": {
        "id": "v65oG7ZeJ8eb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oELpk30MXAwX"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\"\n",
        "groq_api_key=os.environ[\"GROQ_API_KEY\"]\n",
        "\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n",
        "\n",
        "llm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ['HF_TOKEN']=hf_token\n",
        "hf_token=os.environ['HF_TOKEN']\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "7FmNAmaBZEgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "rvN51OKKa62F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**overview:**\n",
        "- we loaded a webpage and used **BeautifulSoup** to **parse and filter the HTML content**. By targeting specific class names, we extracted only the relevant sections like the title, header, and main content, allowing us to focus on useful information while ignoring unnecessary parts of the page."
      ],
      "metadata": {
        "id": "adOKAdbWooZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs=loader.load()\n",
        "docs"
      ],
      "metadata": {
        "id": "l7hC8gpxbx7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "splits=text_splitter.split_documents(docs)\n",
        "vectorstore=Chroma.from_documents(documents=splits,embedding=embeddings)\n",
        "retriever=vectorstore.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "id": "sD1rhPFDcfcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prompt Template\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1qQvwytGdjh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain=create_stuff_documents_chain(llm,prompt)\n",
        "rag_chain=create_retrieval_chain(retriever,question_answer_chain)"
      ],
      "metadata": {
        "id": "YWkGVLGw6A1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "- This setup creates a RAG-based question-answering system that retrieves relevant information and generates concise answers using a language model. It enhances the accuracy and relevance of responses by grounding them in real-world data, making it ideal for tasks like document-based chat, knowledge assistants, and smart search systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FrfOyXKuqKg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke({\"input\":\"What is Self-Reflection\"})\n",
        "response"
      ],
      "metadata": {
        "id": "HzUEp2eP6PRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response['answer']"
      ],
      "metadata": {
        "id": "9drferE66Rzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"input\":\"Howw do we achieve it\"})"
      ],
      "metadata": {
        "id": "yi6TybCb6XC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "rbZfMN0U6cur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
        "history_aware_retriever"
      ],
      "metadata": {
        "id": "XjTYw_Sh6i2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "DTIT-QPg6kym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
        "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
      ],
      "metadata": {
        "id": "wFTd1Lte6pbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage,HumanMessage\n",
        "chat_history=[]\n",
        "question=\"What is Self-Reflection\"\n",
        "response1=rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=response1[\"answer\"])\n",
        "    ]\n",
        ")\n",
        "\n",
        "question2=\"Tell me more about it?\"\n",
        "response2=rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
        "print(response2['answer'])"
      ],
      "metadata": {
        "id": "ZChpgMLP6sGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "p7DYLSB46x-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "- This setup demonstrates a conversational RAG (Retrieval-Augmented Generation) system that can understand and answer questions based not only on retrieved documents but also on previous interactions.\n",
        "- The first user question is answered using relevant content retrieved from a knowledge source. That question and answer are saved as chat history.\n",
        "- When the user asks a follow-up question like ‚ÄúTell me more about it?‚Äù, the system uses the stored history to understand what ‚Äúit‚Äù refers to and generate a meaningful, context-aware answer.\n",
        "-This shows how combining document retrieval with chat history enables more natural, coherent, and intelligent multi-turn conversations."
      ],
      "metadata": {
        "id": "AIVumzPDsGA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "id": "fUkGx2T_62qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is Task Decomposition?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    },  # constructs a key \"abc123\" in `store`.\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "id": "4L6v9Vg666Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"who is the president of US ?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "id": "jDlyc7qhCVbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What are common ways of doing it?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "id": "YknxYr1i6-lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "id": "dc74eSCB7CZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:**\n",
        "- The **conversational AI** was able to answer the question ‚ÄúWhat are common ways of doing it?‚Äù because it had access to previous context in the chat history‚Äîspecifically, a prior question about ‚ÄúTask Decomposition.‚Äù\n",
        "- This allowed the model to understand what ‚Äúit‚Äù referred to and retrieve relevant information from the external source (the linked article) that discussed strategies for task decomposition.\n",
        "- In contrast, when asked ‚Äú**Who is the president of the US**?‚Äù, the model responded ‚Äú**I don‚Äôt know**‚Äù because that information was not present in the external document. RAG systems can only answer based on what they retrieve, so if the knowledge isn‚Äôt in the source documents, the model cannot provide an answer‚Äîeven if the question is clear."
      ],
      "metadata": {
        "id": "pjgZrdZDwji0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mini AI Study Chatbot  with RAG + Chat History**\n",
        "\n"
      ],
      "metadata": {
        "id": "k1qN86gkyUx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "session_id = \"student_ai_study\"\n",
        "\n",
        "question1 = \"What is Task Decomposition?\"\n",
        "response1 = conversational_rag_chain.invoke(\n",
        "    {\"input\": question1},\n",
        "    config={\"configurable\": {\"session_id\": session_id}},\n",
        ")\n",
        "print(\"üë©‚Äçüéì\", question1)\n",
        "print(\"ü§ñ\", response1[\"answer\"])\n",
        "\n",
        "\n",
        "question2 = \"What are some common ways of doing it?\"\n",
        "response2 = conversational_rag_chain.invoke(\n",
        "    {\"input\": question2},\n",
        "    config={\"configurable\": {\"session_id\": session_id}},\n",
        ")\n",
        "print(\"\\nüë©‚Äçüéì\", question2)\n",
        "print(\"ü§ñ\", response2[\"answer\"])\n",
        "\n",
        "question3 = \"What evidence supports the existence of dark matter in galaxies?\"\n",
        "response3 = conversational_rag_chain.invoke(\n",
        "    {\"input\": question3},\n",
        "    config={\"configurable\": {\"session_id\": session_id}},\n",
        ")\n",
        "print(\"\\nüë©‚Äçüéì\", question3)\n",
        "print(\"ü§ñ\", response3[\"answer\"])\n"
      ],
      "metadata": {
        "id": "blPjVvt9yZ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "we have built a sophisticated Q&A chatbot with several advanced capabilities:\n",
        "\n",
        "1. **Context-Aware Conversations**: Your chatbot remembers previous exchanges and maintains coherent dialogues\n",
        "2. **Multi-User Support**: The system can handle multiple conversation sessions simultaneously\n",
        "3. **Multilingual Communication**: Your assistant can converse in various languages\n",
        "4. **Memory Management**: The chatbot efficiently handles conversation history\n",
        "5. **Knowledge Integration**: External information can be\n",
        "\n",
        "\n",
        "# **Final Thoughts**\n",
        "\n",
        "The field of conversational AI is evolving rapidly, with new models and techniques emerging regularly. By understanding the fundamental components covered in this project, we have built a strong foundation that will allow us to adapt to these changes and create increasingly sophisticated AI applications.\n",
        "\n",
        "Building effective AI systems is an iterative process that benefits from continuous testing and refinement. As we  continue to experiment and learn, the  chatbots will become more capable, helpful, and natural in their interactions.\n",
        "\n",
        "**Thank you for exploring this fascinating field with us!**"
      ],
      "metadata": {
        "id": "jYMuglmPV7zg"
      }
    }
  ]
}